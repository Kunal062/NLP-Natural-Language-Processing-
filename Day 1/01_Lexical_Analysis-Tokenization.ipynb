{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\administrator.dai-pc2\\anaconda3\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: spacy in c:\\users\\administrator.dai-pc2\\anaconda3\\lib\\site-packages (3.7.4)\n",
      "Requirement already satisfied: textblob in c:\\users\\administrator.dai-pc2\\anaconda3\\lib\\site-packages (0.18.0.post0)\n",
      "Requirement already satisfied: click in c:\\users\\administrator.dai-pc2\\anaconda3\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\administrator.dai-pc2\\anaconda3\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\administrator.dai-pc2\\anaconda3\\lib\\site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\administrator.dai-pc2\\anaconda3\\lib\\site-packages (from nltk) (4.66.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\administrator.dai-pc2\\anaconda3\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\administrator.dai-pc2\\anaconda3\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\administrator.dai-pc2\\anaconda3\\lib\\site-packages (from spacy) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\administrator.dai-pc2\\anaconda3\\lib\\site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\administrator.dai-pc2\\anaconda3\\lib\\site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in c:\\users\\administrator.dai-pc2\\anaconda3\\lib\\site-packages (from spacy) (8.2.3)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\administrator.dai-pc2\\anaconda3\\lib\\site-packages (from spacy) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\administrator.dai-pc2\\anaconda3\\lib\\site-packages (from spacy) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\administrator.dai-pc2\\anaconda3\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in c:\\users\\administrator.dai-pc2\\anaconda3\\lib\\site-packages (from spacy) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\administrator.dai-pc2\\anaconda3\\lib\\site-packages (from spacy) (0.9.4)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\administrator.dai-pc2\\anaconda3\\lib\\site-packages (from spacy) (5.2.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\administrator.dai-pc2\\anaconda3\\lib\\site-packages (from spacy) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\administrator.dai-pc2\\anaconda3\\lib\\site-packages (from spacy) (1.10.15)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\administrator.dai-pc2\\anaconda3\\lib\\site-packages (from spacy) (3.1.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\administrator.dai-pc2\\anaconda3\\lib\\site-packages (from spacy) (68.2.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\administrator.dai-pc2\\anaconda3\\lib\\site-packages (from spacy) (23.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\administrator.dai-pc2\\anaconda3\\lib\\site-packages (from spacy) (3.4.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\administrator.dai-pc2\\anaconda3\\lib\\site-packages (from spacy) (1.26.4)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\administrator.dai-pc2\\anaconda3\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\administrator.dai-pc2\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\administrator.dai-pc2\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\administrator.dai-pc2\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\administrator.dai-pc2\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\administrator.dai-pc2\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.2.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\administrator.dai-pc2\\anaconda3\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\administrator.dai-pc2\\anaconda3\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\administrator.dai-pc2\\anaconda3\\lib\\site-packages (from tqdm->nltk) (0.4.6)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in c:\\users\\administrator.dai-pc2\\anaconda3\\lib\\site-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\administrator.dai-pc2\\anaconda3\\lib\\site-packages (from jinja2->spacy) (2.1.3)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in c:\\users\\administrator.dai-pc2\\anaconda3\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.1.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk spacy textblob -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Administrator.DAI-\n",
      "[nltk_data]     PC2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Administrator.DAI-\n",
      "[nltk_data]     PC2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Administrator.DAI-\n",
      "[nltk_data]     PC2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Administrator.DAI-\n",
      "[nltk_data]     PC2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to C:\\Users\\Administrator.DAI-\n",
      "[nltk_data]     PC2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package indian to C:\\Users\\Administrator.DAI-\n",
      "[nltk_data]     PC2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package indian is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\Administrator.DAI-\n",
      "[nltk_data]     PC2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')  # Tokenization\n",
    "nltk.download('stopwords')  # StopWords Removal\n",
    "nltk.download('averaged_perceptron_tagger')  # Part-of-Speech Tagging\n",
    "nltk.download('wordnet')  # Wordnet database and lemmatization\n",
    "nltk.download('omw-1.4')  # Stemming\n",
    "nltk.download('indian')  # Indian Language POS Tagging\n",
    "nltk.download('maxent_ne_chunker')  # Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = 'They told that their ages are 25 27 and 31 respectively.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = sent.split()\n",
    "age=[]\n",
    "for i in txt:\n",
    "    if(i.isdigit()):\n",
    "        age.append(int(i))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27.666666666666668"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(age)/len(age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27.666666666666668"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ages = [int(word) for word in sent.split(' ') if word.isdigit()]\n",
    "sum(ages) / len(ages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27.666666666666668"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([int(word) for word in sent.split() if word.isdigit()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = 'Hello friends! How are you? Welcome to Python Programming.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the functions\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello friends!', 'How are you?', 'Welcome to Python Programming.']"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# segmentation\n",
    "sent_tokenize(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'friends',\n",
       " '!',\n",
       " 'How',\n",
       " 'are',\n",
       " 'you',\n",
       " '?',\n",
       " 'Welcome',\n",
       " 'to',\n",
       " 'Python',\n",
       " 'Programming',\n",
       " '.']"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25.0"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "count = 0\n",
    "for i in word_tokenize(sent):\n",
    "    if i in string.punctuation:\n",
    "        count += 1\n",
    "\n",
    "(count/len(word_tokenize(sent)))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25.0"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "percent = [word for word in word_tokenize(sent) if word in string.punctuation]\n",
    "(len(percent)/len(word_tokenize(sent)))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "punct_count = len([word for word in word_tokenize(sent) if not word.isalnum()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "punct_count / len(word_tokenize(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = ord('v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0b1110110'"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bin(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.getsizeof('g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function getsizeof in module sys:\n",
      "\n",
      "getsizeof(...)\n",
      "    getsizeof(object [, default]) -> int\n",
      "    \n",
      "    Return the size of object in bytes.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(sys.getsizeof)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ā'"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chr(257)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u0000\n",
      "\n",
      "\n",
      "\u0014\n",
      "\u001e\n",
      "(\n",
      "2\n",
      "<\n",
      "F\n",
      "P\n",
      "Z\n",
      "d\n",
      "n\n",
      "x\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "ª\n",
      "´\n",
      "¾\n",
      "È\n",
      "Ò\n",
      "Ü\n",
      "æ\n",
      "ð\n",
      "ú\n",
      "Ą\n",
      "Ď\n",
      "Ę\n",
      "Ģ\n",
      "Ĭ\n",
      "Ķ\n",
      "ŀ\n",
      "Ŋ\n",
      "Ŕ\n",
      "Ş\n",
      "Ũ\n",
      "Ų\n",
      "ż\n",
      "Ɔ\n",
      "Ɛ\n",
      "ƚ\n",
      "Ƥ\n",
      "Ʈ\n",
      "Ƹ\n",
      "ǂ\n",
      "ǌ\n",
      "ǖ\n",
      "Ǡ\n",
      "Ǫ\n",
      "Ǵ\n",
      "Ǿ\n",
      "Ȉ\n",
      "Ȓ\n",
      "Ȝ\n",
      "Ȧ\n",
      "Ȱ\n",
      "Ⱥ\n",
      "Ʉ\n",
      "Ɏ\n",
      "ɘ\n",
      "ɢ\n",
      "ɬ\n",
      "ɶ\n",
      "ʀ\n",
      "ʊ\n",
      "ʔ\n",
      "ʞ\n",
      "ʨ\n",
      "ʲ\n",
      "ʼ\n",
      "ˆ\n",
      "ː\n",
      "˚\n",
      "ˤ\n",
      "ˮ\n",
      "˸\n",
      "̂\n",
      "̌\n",
      "̖\n",
      "̠\n",
      "̪\n",
      "̴\n",
      "̾\n",
      "͈\n",
      "͒\n",
      "͜\n",
      "ͦ\n",
      "Ͱ\n",
      "ͺ\n",
      "΄\n",
      "Ύ\n",
      "Θ\n",
      "΢\n",
      "ά\n",
      "ζ\n",
      "π\n",
      "ϊ\n",
      "ϔ\n",
      "Ϟ\n",
      "Ϩ\n",
      "ϲ\n",
      "ϼ\n",
      "І\n",
      "А\n",
      "К\n",
      "Ф\n",
      "Ю\n",
      "и\n",
      "т\n",
      "ь\n",
      "і\n",
      "Ѡ\n",
      "Ѫ\n",
      "Ѵ\n",
      "Ѿ\n",
      "҈\n",
      "Ғ\n",
      "Ҝ\n",
      "Ҧ\n",
      "Ұ\n",
      "Һ\n",
      "ӄ\n",
      "ӎ\n",
      "Ә\n",
      "Ӣ\n",
      "Ӭ\n",
      "Ӷ\n",
      "Ԁ\n",
      "Ԋ\n",
      "Ԕ\n",
      "Ԟ\n",
      "Ԩ\n",
      "Բ\n",
      "Լ\n",
      "Ն\n",
      "Ր\n",
      "՚\n",
      "դ\n",
      "ծ\n",
      "ո\n",
      "ւ\n",
      "֌\n",
      "֖\n",
      "֠\n",
      "֪\n",
      "ִ\n",
      "־\n",
      "׈\n",
      "ג\n",
      "ל\n",
      "צ\n",
      "װ\n",
      "׺\n",
      "؄\n",
      "؎\n",
      "ؘ\n",
      "آ\n",
      "ج\n",
      "ض\n",
      "ـ\n",
      "ي\n",
      "ٔ\n",
      "ٞ\n",
      "٨\n",
      "ٲ\n",
      "ټ\n",
      "چ\n",
      "ڐ\n",
      "ښ\n",
      "ڤ\n",
      "ڮ\n",
      "ڸ\n",
      "ۂ\n",
      "ی\n",
      "ۖ\n",
      "۠\n",
      "۪\n",
      "۴\n",
      "۾\n",
      "܈\n",
      "ܒ\n",
      "ܜ\n",
      "ܦ\n",
      "ܰ\n",
      "ܺ\n",
      "݄\n",
      "ݎ\n",
      "ݘ\n",
      "ݢ\n",
      "ݬ\n",
      "ݶ\n",
      "ހ\n",
      "ފ\n",
      "ޔ\n",
      "ޞ\n",
      "ި\n",
      "޲\n",
      "޼\n",
      "߆\n",
      "ߐ\n",
      "ߚ\n",
      "ߤ\n",
      "߮\n",
      "߸\n",
      "ࠂ\n",
      "ࠌ\n",
      "ࠖ\n",
      "ࠠ\n",
      "ࠪ\n",
      "࠴\n",
      "࠾\n",
      "ࡈ\n",
      "ࡒ\n",
      "࡜\n",
      "ࡦ\n",
      "ࡰ\n",
      "ࡺ\n",
      "ࢄ\n",
      "ࢎ\n",
      "࢘\n",
      "ࢢ\n",
      "ࢬ\n",
      "ࢶ\n",
      "ࣀ\n",
      "࣊\n",
      "ࣔ\n",
      "ࣞ\n",
      "ࣨ\n",
      "ࣲ\n",
      "ࣼ\n",
      "आ\n",
      "ऐ\n",
      "च\n",
      "त\n",
      "म\n",
      "स\n",
      "ू\n",
      "ौ\n",
      "ॖ\n",
      "ॠ\n",
      "४\n",
      "ॴ\n",
      "ॾ\n",
      "ঈ\n",
      "঒\n",
      "জ\n",
      "দ\n",
      "র\n",
      "঺\n",
      "ৄ\n",
      "ৎ\n",
      "৘\n",
      "ৢ\n",
      "৬\n",
      "৶\n",
      "਀\n",
      "ਊ\n",
      "ਔ\n",
      "ਞ\n",
      "ਨ\n",
      "ਲ\n",
      "਼\n",
      "੆\n",
      "੐\n",
      "ਗ਼\n",
      "੤\n",
      "੮\n",
      "੸\n",
      "ં\n",
      "ઌ\n",
      "ખ\n",
      "ઠ\n",
      "પ\n",
      "઴\n",
      "ા\n",
      "ૈ\n",
      "૒\n",
      "૜\n",
      "૦\n",
      "૰\n",
      "ૺ\n",
      "଄\n",
      "଎\n",
      "ଘ\n",
      "ଢ\n",
      "ବ\n",
      "ଶ\n",
      "ୀ\n",
      "୊\n",
      "୔\n",
      "୞\n",
      "୨\n",
      "୲\n",
      "୼\n",
      "ஆ\n",
      "ஐ\n",
      "ச\n",
      "த\n",
      "ம\n",
      "ஸ\n",
      "ூ\n",
      "ௌ\n",
      "௖\n",
      "௠\n",
      "௪\n",
      "௴\n",
      "௾\n",
      "ఈ\n",
      "ఒ\n",
      "జ\n",
      "ద\n",
      "ర\n",
      "఺\n",
      "ౄ\n",
      "౎\n",
      "ౘ\n",
      "ౢ\n",
      "౬\n",
      "౶\n",
      "ಀ\n",
      "ಊ\n",
      "ಔ\n",
      "ಞ\n",
      "ನ\n",
      "ಲ\n",
      "಼\n",
      "ೆ\n",
      "೐\n",
      "೚\n",
      "೤\n",
      "೮\n",
      "೸\n",
      "ം\n",
      "ഌ\n",
      "ഖ\n",
      "ഠ\n",
      "പ\n",
      "ഴ\n",
      "ാ\n",
      "ൈ\n",
      "൒\n",
      "൜\n",
      "൦\n",
      "൰\n",
      "ൺ\n",
      "඄\n",
      "ඎ\n",
      "඘\n",
      "ජ\n",
      "ඬ\n",
      "බ\n",
      "ව\n",
      "්\n",
      "ු\n",
      "ෞ\n",
      "෨\n",
      "ෲ\n",
      "෼\n",
      "ฆ\n",
      "ฐ\n",
      "บ\n",
      "ฤ\n",
      "ฮ\n",
      "ุ\n",
      "โ\n",
      "์\n",
      "๖\n",
      "๠\n",
      "๪\n",
      "๴\n",
      "๾\n",
      "ຈ\n",
      "ຒ\n",
      "ຜ\n",
      "຦\n",
      "ະ\n",
      "຺\n",
      "ໄ\n",
      "໎\n",
      "໘\n",
      "໢\n",
      "໬\n",
      "໶\n",
      "ༀ\n",
      "༊\n",
      "༔\n",
      "༞\n",
      "༨\n",
      "༲\n",
      "༼\n",
      "ཆ\n",
      "ཐ\n",
      "ཚ\n",
      "ཤ\n",
      "཮\n",
      "ླྀ\n",
      "ྂ\n",
      "ྌ\n",
      "ྖ\n",
      "ྠ\n",
      "ྪ\n",
      "ྴ\n",
      "྾\n",
      "࿈\n",
      "࿒\n",
      "࿜\n",
      "࿦\n",
      "࿰\n",
      "࿺\n",
      "င\n",
      "ဎ\n",
      "ဘ\n",
      "ဢ\n",
      "ာ\n",
      "ံ\n",
      "၀\n",
      "၊\n",
      "ၔ\n",
      "ၞ\n",
      "ၨ\n",
      "ၲ\n",
      "ၼ\n",
      "ႆ\n",
      "႐\n",
      "ႚ\n",
      "Ⴄ\n",
      "Ⴎ\n",
      "Ⴘ\n",
      "Ⴢ\n",
      "჌\n",
      "ზ\n",
      "რ\n",
      "ც\n",
      "ჴ\n",
      "ჾ\n",
      "ᄈ\n",
      "ᄒ\n",
      "ᄜ\n",
      "ᄦ\n",
      "ᄰ\n",
      "ᄺ\n",
      "ᅄ\n",
      "ᅎ\n",
      "ᅘ\n",
      "ᅢ\n",
      "ᅬ\n",
      "ᅶ\n",
      "ᆀ\n",
      "ᆊ\n",
      "ᆔ\n",
      "ᆞ\n",
      "ᆨ\n",
      "ᆲ\n",
      "ᆼ\n",
      "ᇆ\n",
      "ᇐ\n",
      "ᇚ\n",
      "ᇤ\n",
      "ᇮ\n",
      "ᇸ\n",
      "ሂ\n",
      "ሌ\n",
      "ሖ\n",
      "ሠ\n",
      "ሪ\n",
      "ሴ\n",
      "ሾ\n",
      "ቈ\n",
      "ቒ\n",
      "ቜ\n",
      "ቦ\n",
      "ተ\n",
      "ቺ\n",
      "ኄ\n",
      "኎\n",
      "ኘ\n",
      "ኢ\n",
      "ኬ\n",
      "኶\n",
      "ዀ\n",
      "ዊ\n",
      "ዔ\n",
      "ዞ\n",
      "የ\n",
      "ዲ\n",
      "ዼ\n",
      "ጆ\n",
      "ጐ\n",
      "ጚ\n",
      "ጤ\n",
      "ጮ\n",
      "ጸ\n",
      "ፂ\n",
      "ፌ\n",
      "ፖ\n",
      "፠\n",
      "፪\n",
      "፴\n",
      "፾\n",
      "ᎈ\n",
      "᎒\n",
      "᎜\n",
      "Ꭶ\n",
      "Ꮀ\n",
      "Ꮊ\n",
      "Ꮔ\n",
      "Ꮞ\n",
      "Ꮨ\n",
      "Ꮲ\n",
      "Ꮼ\n",
      "᏶\n",
      "᐀\n",
      "ᐊ\n",
      "ᐔ\n",
      "ᐞ\n",
      "ᐨ\n",
      "ᐲ\n",
      "ᐼ\n",
      "ᑆ\n",
      "ᑐ\n",
      "ᑚ\n",
      "ᑤ\n",
      "ᑮ\n",
      "ᑸ\n",
      "ᒂ\n",
      "ᒌ\n",
      "ᒖ\n",
      "ᒠ\n",
      "ᒪ\n",
      "ᒴ\n",
      "ᒾ\n",
      "ᓈ\n",
      "ᓒ\n",
      "ᓜ\n",
      "ᓦ\n",
      "ᓰ\n",
      "ᓺ\n",
      "ᔄ\n",
      "ᔎ\n",
      "ᔘ\n",
      "ᔢ\n",
      "ᔬ\n",
      "ᔶ\n",
      "ᕀ\n",
      "ᕊ\n",
      "ᕔ\n",
      "ᕞ\n",
      "ᕨ\n",
      "ᕲ\n",
      "ᕼ\n",
      "ᖆ\n",
      "ᖐ\n",
      "ᖚ\n",
      "ᖤ\n",
      "ᖮ\n",
      "ᖸ\n",
      "ᗂ\n",
      "ᗌ\n",
      "ᗖ\n",
      "ᗠ\n",
      "ᗪ\n",
      "ᗴ\n",
      "ᗾ\n",
      "ᘈ\n",
      "ᘒ\n",
      "ᘜ\n",
      "ᘦ\n",
      "ᘰ\n",
      "ᘺ\n",
      "ᙄ\n",
      "ᙎ\n",
      "ᙘ\n",
      "ᙢ\n",
      "ᙬ\n",
      "ᙶ\n",
      " \n",
      "ᚊ\n",
      "ᚔ\n",
      "᚞\n",
      "ᚨ\n",
      "ᚲ\n",
      "ᚼ\n",
      "ᛆ\n",
      "ᛐ\n",
      "ᛚ\n",
      "ᛤ\n",
      "ᛮ\n",
      "ᛸ\n",
      "ᜂ\n",
      "ᜌ\n",
      "᜖\n",
      "ᜠ\n",
      "ᜪ\n",
      "᜴\n",
      "᜾\n",
      "ᝈ\n",
      "ᝒ\n",
      "᝜\n",
      "ᝦ\n",
      "ᝰ\n",
      "᝺\n",
      "ង\n",
      "ណ\n",
      "ម\n",
      "អ\n",
      "ឬ\n",
      "ា\n",
      "ៀ\n",
      "៊\n",
      "។\n",
      "៞\n",
      "៨\n",
      "៲\n",
      "៼\n",
      "᠆\n",
      "᠐\n",
      "᠚\n",
      "ᠤ\n",
      "ᠮ\n",
      "ᠸ\n",
      "ᡂ\n",
      "ᡌ\n",
      "ᡖ\n",
      "ᡠ\n",
      "ᡪ\n",
      "ᡴ\n",
      "᡾\n",
      "ᢈ\n",
      "ᢒ\n",
      "ᢜ\n",
      "ᢦ\n",
      "ᢰ\n",
      "ᢺ\n",
      "ᣄ\n",
      "ᣎ\n",
      "ᣘ\n",
      "ᣢ\n",
      "ᣬ\n",
      "᣶\n",
      "ᤀ\n",
      "ᤊ\n",
      "ᤔ\n",
      "ᤞ\n",
      "ᤨ\n",
      "ᤲ\n",
      "᤼\n",
      "᥆\n",
      "ᥐ\n",
      "ᥚ\n",
      "ᥤ\n",
      "᥮\n",
      "᥸\n",
      "ᦂ\n",
      "ᦌ\n",
      "ᦖ\n",
      "ᦠ\n",
      "ᦪ\n",
      "ᦴ\n",
      "ᦾ\n",
      "ᧈ\n",
      "᧒\n",
      "᧜\n",
      "᧦\n",
      "᧰\n",
      "᧺\n",
      "ᨄ\n",
      "ᨎ\n",
      "ᨘ\n",
      "ᨢ\n",
      "ᨬ\n",
      "ᨶ\n",
      "ᩀ\n",
      "ᩊ\n",
      "ᩔ\n",
      "ᩞ\n",
      "ᩨ\n",
      "ᩲ\n",
      "᩼\n",
      "᪆\n",
      "᪐\n",
      "᪚\n",
      "᪤\n",
      "᪮\n",
      "᪸\n",
      "᫂\n",
      "ᫌ\n",
      "᫖\n",
      "᫠\n",
      "᫪\n",
      "᫴\n",
      "᫾\n",
      "ᬈ\n",
      "ᬒ\n",
      "ᬜ\n",
      "ᬦ\n",
      "ᬰ\n",
      "ᬺ\n",
      "᭄\n",
      "᭎\n",
      "᭘\n",
      "᭢\n",
      "᭬\n",
      "᭶\n",
      "ᮀ\n",
      "ᮊ\n",
      "ᮔ\n",
      "ᮞ\n",
      "ᮨ\n",
      "᮲\n",
      "ᮼ\n",
      "ᯆ\n",
      "ᯐ\n",
      "ᯚ\n",
      "ᯤ\n",
      "ᯮ\n",
      "᯸\n",
      "ᰂ\n",
      "ᰌ\n",
      "ᰖ\n",
      "ᰠ\n",
      "ᰪ\n",
      "ᰴ\n",
      "᰾\n",
      "᱈\n",
      "᱒\n",
      "ᱜ\n",
      "ᱦ\n",
      "ᱰ\n",
      "ᱺ\n",
      "ᲄ\n",
      "᲎\n",
      "Ი\n",
      "Ტ\n",
      "Წ\n",
      "Ჶ\n",
      "᳀\n",
      "᳊\n",
      "᳔\n",
      "᳞\n",
      "᳨\n",
      "ᳲ\n",
      "᳼\n",
      "ᴆ\n",
      "ᴐ\n",
      "ᴚ\n",
      "ᴤ\n",
      "ᴮ\n",
      "ᴸ\n",
      "ᵂ\n",
      "ᵌ\n",
      "ᵖ\n",
      "ᵠ\n",
      "ᵪ\n",
      "ᵴ\n",
      "ᵾ\n",
      "ᶈ\n",
      "ᶒ\n",
      "ᶜ\n",
      "ᶦ\n",
      "ᶰ\n",
      "ᶺ\n",
      "᷄\n",
      "᷎\n",
      "ᷘ\n",
      "ᷢ\n",
      "ᷬ\n",
      "᷶\n",
      "Ḁ\n",
      "Ḋ\n",
      "Ḕ\n",
      "Ḟ\n",
      "Ḩ\n",
      "Ḳ\n",
      "Ḽ\n",
      "Ṇ\n",
      "Ṑ\n",
      "Ṛ\n",
      "Ṥ\n",
      "Ṯ\n",
      "Ṹ\n",
      "Ẃ\n",
      "Ẍ\n",
      "ẖ\n",
      "Ạ\n",
      "Ẫ\n",
      "Ẵ\n",
      "Ế\n",
      "Ỉ\n",
      "Ồ\n",
      "Ờ\n",
      "Ủ\n",
      "Ự\n",
      "Ỻ\n",
      "ἄ\n",
      "Ἆ\n",
      "Ἐ\n",
      "ἢ\n",
      "Ἤ\n",
      "ἶ\n",
      "ὀ\n",
      "Ὂ\n",
      "ὔ\n",
      "὞\n",
      "Ὠ\n",
      "ὲ\n",
      "ὼ\n",
      "ᾆ\n",
      "ᾐ\n",
      "ᾚ\n",
      "ᾤ\n",
      "ᾮ\n",
      "Ᾰ\n",
      "ῂ\n",
      "ῌ\n",
      "ῖ\n",
      "ῠ\n",
      "Ὺ\n",
      "ῴ\n",
      "῾\n",
      " \n",
      "‒\n",
      "“\n",
      "…\n",
      "‰\n",
      "›\n",
      "⁄\n",
      "⁎\n",
      "⁘\n",
      "⁢\n",
      "⁬\n",
      "⁶\n",
      "₀\n",
      "₊\n",
      "ₔ\n",
      "₞\n",
      "₨\n",
      "₲\n",
      "₼\n",
      "⃆\n",
      "⃐\n",
      "⃚\n",
      "⃤\n",
      "⃮\n",
      "⃸\n",
      "ℂ\n",
      "ℌ\n",
      "№\n",
      "℠\n",
      "K\n",
      "ℴ\n",
      "ℾ\n",
      "ⅈ\n",
      "⅒\n",
      "⅜\n",
      "Ⅶ\n",
      "ⅰ\n",
      "ⅺ\n",
      "ↄ\n",
      "↎\n",
      "↘\n",
      "↢\n",
      "↬\n",
      "↶\n",
      "⇀\n",
      "⇊\n",
      "⇔\n",
      "⇞\n",
      "⇨\n",
      "⇲\n",
      "⇼\n",
      "∆\n",
      "∐\n",
      "√\n",
      "∤\n",
      "∮\n",
      "∸\n",
      "≂\n",
      "≌\n",
      "≖\n",
      "≠\n",
      "≪\n",
      "≴\n",
      "≾\n",
      "⊈\n",
      "⊒\n",
      "⊜\n",
      "⊦\n",
      "⊰\n",
      "⊺\n",
      "⋄\n",
      "⋎\n",
      "⋘\n",
      "⋢\n",
      "⋬\n",
      "⋶\n",
      "⌀\n",
      "⌊\n",
      "⌔\n",
      "⌞\n",
      "⌨\n",
      "⌲\n",
      "⌼\n",
      "⍆\n",
      "⍐\n",
      "⍚\n",
      "⍤\n",
      "⍮\n",
      "⍸\n",
      "⎂\n",
      "⎌\n",
      "⎖\n",
      "⎠\n",
      "⎪\n",
      "⎴\n",
      "⎾\n",
      "⏈\n",
      "⏒\n",
      "⏜\n",
      "⏦\n",
      "⏰\n",
      "⏺\n",
      "␄\n",
      "␎\n",
      "␘\n",
      "␢\n",
      "␬\n",
      "␶\n",
      "⑀\n",
      "⑊\n",
      "⑔\n",
      "⑞\n",
      "⑨\n",
      "⑲\n",
      "⑼\n",
      "⒆\n",
      "⒐\n",
      "⒚\n",
      "⒤\n",
      "⒮\n",
      "Ⓒ\n",
      "Ⓜ\n",
      "Ⓦ\n",
      "ⓖ\n",
      "ⓠ\n",
      "⓪\n",
      "⓴\n",
      "⓾\n",
      "┈\n",
      "┒\n",
      "├\n",
      "┦\n",
      "┰\n",
      "┺\n",
      "╄\n",
      "╎\n",
      "╘\n",
      "╢\n",
      "╬\n",
      "╶\n",
      "▀\n",
      "▊\n",
      "▔\n",
      "▞\n",
      "▨\n",
      "▲\n",
      "▼\n",
      "◆\n",
      "◐\n",
      "◚\n",
      "◤\n",
      "◮\n",
      "◸\n",
      "☂\n",
      "☌\n",
      "☖\n",
      "☠\n",
      "☪\n",
      "☴\n",
      "☾\n",
      "♈\n",
      "♒\n",
      "♜\n",
      "♦\n",
      "♰\n",
      "♺\n",
      "⚄\n",
      "⚎\n",
      "⚘\n",
      "⚢\n",
      "⚬\n",
      "⚶\n",
      "⛀\n",
      "⛊\n",
      "⛔\n",
      "⛞\n",
      "⛨\n",
      "⛲\n",
      "⛼\n",
      "✆\n"
     ]
    }
   ],
   "source": [
    "a = 10\n",
    "for i in range(1000):\n",
    "    print(chr(a*i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'आ'"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char = '\\u0906'  # Hexadecimal value\n",
    "char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "c =ord('आ') # decimal Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0x906'"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hex(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'अ'"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chr(0x905)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['कुणाल', 'उके']"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name = 'कुणाल उके'\n",
    "name.split()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name.startswith('क')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'कोणाल उके'"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name.replace('कु','को')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name.find('णा')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['कुणाल', 'योगेश', 'मानसी', 'रिया', 'आदित्य', 'कणिका ' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "कुणाल\n",
      "कणिका \n"
     ]
    }
   ],
   "source": [
    "for name in names:\n",
    "    if name.startswith('क'):\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtext = 'हिन्दी विकिपीडिया, विकिपीडिया का हिन्दी भाषा का संस्करण है, जिसका स्वामित्व विकिमीडिया संस्थापन के पास है। हिन्दी संस्करण जुलाई 2003 में आरंभ (आरम्भ) किया गया था और जून 2024 तक इस पर 1,61,709 लेख और लगभग 8,14,599 पंजीकृत सदस्य हैं।[1] [2] 30 अगस्त 2011 के दिन यह एक लाख लेखों का आँकड़ा पार करने वाला प्रथम भारतीय भाषा विकिपीडिया बना। यह लेखों की संख्या, सक्रिय सदस्यों, प्रयोक्ताओं की संख्या, सम्पादनों इत्यादि के आधार पर भारतीय भाषाओं में उपलब्ध विकिपीडिया का सबसे बड़ा संस्करण है और सभी संस्करणों में पचपनवाँ है। और इसे मुख्यतः हिन्दीभाषी लोगों की आवश्यकताओं की पूर्ति के लिए बनाया गया था। चूँकि हिन्दी विकिपीडिया इण्डिक स्क्रिप्ट (देवनागरी) का प्रयोग करता है इसलिए इसमें जटिल पाठ प्रतिपादन सहायक की आवश्यकता पड़ती है। विकिपीडिया पर ध्वन्यात्मक रोमन वर्णमाला परिवर्तक उपलब्ध है, इसलिए बिना किसी विशेष हिन्दी टाइपिंग सॉफ्टवेर डाउनलोड किये रोमन कुंजीपटल का उपयोग देवनागरी में टंकण करने के लिए किया जा सकता है।'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'हिन्दी विकिपीडिया, विकिपीडिया का हिन्दी भाषा का संस्करण है, जिसका स्वामित्व विकिमीडिया संस्थापन के पास है। हिन्दी संस्करण जुलाई 2003 में आरंभ (आरम्भ) किया गया था और जून 2024 तक इस पर 1,61,709 लेख और लगभग 8,14,599 पंजीकृत सदस्य हैं।[1] [2] 30 अगस्त 2011 के दिन यह एक लाख लेखों का आँकड़ा पार करने वाला प्रथम भारतीय भाषा विकिपीडिया बना। यह लेखों की संख्या, सक्रिय सदस्यों, प्रयोक्ताओं की संख्या, सम्पादनों इत्यादि के आधार पर भारतीय भाषाओं में उपलब्ध विकिपीडिया का सबसे बड़ा संस्करण है और सभी संस्करणों में पचपनवाँ है। और इसे मुख्यतः हिन्दीभाषी लोगों की आवश्यकताओं की पूर्ति के लिए बनाया गया था। चूँकि हिन्दी विकिपीडिया इण्डिक स्क्रिप्ट (देवनागरी) का प्रयोग करता है इसलिए इसमें जटिल पाठ प्रतिपादन सहायक की आवश्यकता पड़ती है। विकिपीडिया पर ध्वन्यात्मक रोमन वर्णमाला परिवर्तक उपलब्ध है, इसलिए बिना किसी विशेष हिन्दी टाइपिंग सॉफ्टवेर डाउनलोड किये रोमन कुंजीपटल का उपयोग देवनागरी में टंकण करने के लिए किया जा सकता है।'"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['हिन्दी',\n",
       " 'विकिपीडिया',\n",
       " ',',\n",
       " 'विकिपीडिया',\n",
       " 'का',\n",
       " 'हिन्दी',\n",
       " 'भाषा',\n",
       " 'का',\n",
       " 'संस्करण',\n",
       " 'है',\n",
       " ',',\n",
       " 'जिसका',\n",
       " 'स्वामित्व',\n",
       " 'विकिमीडिया',\n",
       " 'संस्थापन',\n",
       " 'के',\n",
       " 'पास',\n",
       " 'है।',\n",
       " 'हिन्दी',\n",
       " 'संस्करण',\n",
       " 'जुलाई',\n",
       " '2003',\n",
       " 'में',\n",
       " 'आरंभ',\n",
       " '(',\n",
       " 'आरम्भ',\n",
       " ')',\n",
       " 'किया',\n",
       " 'गया',\n",
       " 'था',\n",
       " 'और',\n",
       " 'जून',\n",
       " '2024',\n",
       " 'तक',\n",
       " 'इस',\n",
       " 'पर',\n",
       " '1,61,709',\n",
       " 'लेख',\n",
       " 'और',\n",
       " 'लगभग',\n",
       " '8,14,599',\n",
       " 'पंजीकृत',\n",
       " 'सदस्य',\n",
       " 'हैं।',\n",
       " '[',\n",
       " '1',\n",
       " ']',\n",
       " '[',\n",
       " '2',\n",
       " ']',\n",
       " '30',\n",
       " 'अगस्त',\n",
       " '2011',\n",
       " 'के',\n",
       " 'दिन',\n",
       " 'यह',\n",
       " 'एक',\n",
       " 'लाख',\n",
       " 'लेखों',\n",
       " 'का',\n",
       " 'आँकड़ा',\n",
       " 'पार',\n",
       " 'करने',\n",
       " 'वाला',\n",
       " 'प्रथम',\n",
       " 'भारतीय',\n",
       " 'भाषा',\n",
       " 'विकिपीडिया',\n",
       " 'बना।',\n",
       " 'यह',\n",
       " 'लेखों',\n",
       " 'की',\n",
       " 'संख्या',\n",
       " ',',\n",
       " 'सक्रिय',\n",
       " 'सदस्यों',\n",
       " ',',\n",
       " 'प्रयोक्ताओं',\n",
       " 'की',\n",
       " 'संख्या',\n",
       " ',',\n",
       " 'सम्पादनों',\n",
       " 'इत्यादि',\n",
       " 'के',\n",
       " 'आधार',\n",
       " 'पर',\n",
       " 'भारतीय',\n",
       " 'भाषाओं',\n",
       " 'में',\n",
       " 'उपलब्ध',\n",
       " 'विकिपीडिया',\n",
       " 'का',\n",
       " 'सबसे',\n",
       " 'बड़ा',\n",
       " 'संस्करण',\n",
       " 'है',\n",
       " 'और',\n",
       " 'सभी',\n",
       " 'संस्करणों',\n",
       " 'में',\n",
       " 'पचपनवाँ',\n",
       " 'है।',\n",
       " 'और',\n",
       " 'इसे',\n",
       " 'मुख्यतः',\n",
       " 'हिन्दीभाषी',\n",
       " 'लोगों',\n",
       " 'की',\n",
       " 'आवश्यकताओं',\n",
       " 'की',\n",
       " 'पूर्ति',\n",
       " 'के',\n",
       " 'लिए',\n",
       " 'बनाया',\n",
       " 'गया',\n",
       " 'था।',\n",
       " 'चूँकि',\n",
       " 'हिन्दी',\n",
       " 'विकिपीडिया',\n",
       " 'इण्डिक',\n",
       " 'स्क्रिप्ट',\n",
       " '(',\n",
       " 'देवनागरी',\n",
       " ')',\n",
       " 'का',\n",
       " 'प्रयोग',\n",
       " 'करता',\n",
       " 'है',\n",
       " 'इसलिए',\n",
       " 'इसमें',\n",
       " 'जटिल',\n",
       " 'पाठ',\n",
       " 'प्रतिपादन',\n",
       " 'सहायक',\n",
       " 'की',\n",
       " 'आवश्यकता',\n",
       " 'पड़ती',\n",
       " 'है।',\n",
       " 'विकिपीडिया',\n",
       " 'पर',\n",
       " 'ध्वन्यात्मक',\n",
       " 'रोमन',\n",
       " 'वर्णमाला',\n",
       " 'परिवर्तक',\n",
       " 'उपलब्ध',\n",
       " 'है',\n",
       " ',',\n",
       " 'इसलिए',\n",
       " 'बिना',\n",
       " 'किसी',\n",
       " 'विशेष',\n",
       " 'हिन्दी',\n",
       " 'टाइपिंग',\n",
       " 'सॉफ्टवेर',\n",
       " 'डाउनलोड',\n",
       " 'किये',\n",
       " 'रोमन',\n",
       " 'कुंजीपटल',\n",
       " 'का',\n",
       " 'उपयोग',\n",
       " 'देवनागरी',\n",
       " 'में',\n",
       " 'टंकण',\n",
       " 'करने',\n",
       " 'के',\n",
       " 'लिए',\n",
       " 'किया',\n",
       " 'जा',\n",
       " 'सकता',\n",
       " 'है।']"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(mtext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Space Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = open('mydata.txt', 'r')\n",
    "# data.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('mydata.txt', 'r') as file:\n",
    "    text = file.readable()\n",
    "\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello Friends!  How are you?\\n'"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('mydata.txt', 'r') as file:\n",
    "    text = file.readline()\n",
    "\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello Friends!  How are you?\\n',\n",
       " 'Welcome to the world of     Python Programming.']"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('mydata.txt', 'r') as file:\n",
    "    text = file.readlines()\n",
    "\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello Friends!  How are you?\\nWelcome to the world of     Python Programming.'"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('mydata.txt', 'r') as file:\n",
    "    text = file.read()\n",
    "\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Friends!',\n",
       " '',\n",
       " 'How',\n",
       " 'are',\n",
       " 'you?\\nWelcome',\n",
       " 'to',\n",
       " 'the',\n",
       " 'world',\n",
       " 'of',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'Python',\n",
       " 'Programming.']"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import the class\n",
    "from nltk.tokenize import SpaceTokenizer\n",
    "\n",
    "# Create the object\n",
    "tk = SpaceTokenizer()\n",
    "\n",
    "# tokenize the data\n",
    "tk.tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tab Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello Friends! \\tHow are you?\\nWelcome to the world of \\tPython Programming.'"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('mydata.txt', 'r') as file:\n",
    "    text = file.read()\n",
    "\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello Friends! ',\n",
       " 'How are you?\\nWelcome to the world of ',\n",
       " 'Python Programming.']"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TabTokenizer\n",
    "\n",
    "# Create the object\n",
    "tk = TabTokenizer()\n",
    "\n",
    "# tokenize the data\n",
    "tk.tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Line Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello Friends! \\tHow are you?',\n",
       " 'Welcome to the world of \\tPython Programming.']"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the class\n",
    "from nltk.tokenize import LineTokenizer\n",
    "\n",
    "# Create the object\n",
    "tk = LineTokenizer()\n",
    "\n",
    "# tokenize the data\n",
    "tk.tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### White Space Tokenizer -  Space/Tab/Newline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Friends!',\n",
       " 'How',\n",
       " 'are',\n",
       " 'you?',\n",
       " 'Welcome',\n",
       " 'to',\n",
       " 'the',\n",
       " 'world',\n",
       " 'of',\n",
       " 'Python',\n",
       " 'Programming.']"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the class\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "\n",
    "# Create the object\n",
    "tk = WhitespaceTokenizer()\n",
    "\n",
    "# tokenize the data\n",
    "tk.tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MWE Tokenizer - Multi Word Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent1 = '''The Van Rossum is Python creator, visting Pune this week. The development community is very eager to meet Van Rossum.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Van Rossum is Python creator, visting Pune this week. The development community is very eager to meet Van Rossum.\n"
     ]
    }
   ],
   "source": [
    "print(sent1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'Van',\n",
       " 'Rossum',\n",
       " 'is',\n",
       " 'Python',\n",
       " 'creator',\n",
       " ',',\n",
       " 'visting',\n",
       " 'Pune',\n",
       " 'this',\n",
       " 'week',\n",
       " '.',\n",
       " 'The',\n",
       " 'development',\n",
       " 'community',\n",
       " 'is',\n",
       " 'very',\n",
       " 'eager',\n",
       " 'to',\n",
       " 'meet',\n",
       " 'Van',\n",
       " 'Rossum',\n",
       " '.']"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(sent1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'Van Rossum',\n",
       " 'is',\n",
       " 'Python',\n",
       " 'creator',\n",
       " ',',\n",
       " 'visting',\n",
       " 'Pune',\n",
       " 'this',\n",
       " 'week',\n",
       " '.',\n",
       " 'The',\n",
       " 'development',\n",
       " 'community',\n",
       " 'is',\n",
       " 'very',\n",
       " 'eager',\n",
       " 'to',\n",
       " 'meet',\n",
       " 'Van Rossum',\n",
       " '.']"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the class\n",
    "from nltk.tokenize import MWETokenizer\n",
    "\n",
    "# Create the object\n",
    "tk = MWETokenizer(separator=' ')  # Default separator is under score\n",
    "\n",
    "# add Multi Word Expression\n",
    "tk.add_mwe(('Van', 'Rossum'))\n",
    "\n",
    "# tokenize the data\n",
    "tk.tokenize(word_tokenize(sent1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweet Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = 'Hello Friends :)! How are you? Welcome  to the world of Python Programming. :D 😅 :->'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Friends',\n",
       " ':)',\n",
       " '!',\n",
       " 'How',\n",
       " 'are',\n",
       " 'you',\n",
       " '?',\n",
       " 'Welcome',\n",
       " 'to',\n",
       " 'the',\n",
       " 'world',\n",
       " 'of',\n",
       " 'Python',\n",
       " 'Programming',\n",
       " '.',\n",
       " ':D',\n",
       " '😅',\n",
       " ':',\n",
       " '->']"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the class\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "# Create the object\n",
    "tk = TweetTokenizer()\n",
    "\n",
    "# tokenize the data\n",
    "tk.tokenize(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello Friends!🙂\\nWelcome to the world of \\t🐍 Python Programming.🚀'"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('mydata.txt', 'r') as file:\n",
    "    text = file.read()\n",
    "\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Friends',\n",
       " '!',\n",
       " '🙂',\n",
       " 'Welcome',\n",
       " 'to',\n",
       " 'the',\n",
       " 'world',\n",
       " 'of',\n",
       " '🐍',\n",
       " 'Python',\n",
       " 'Programming',\n",
       " '.',\n",
       " '🚀']"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the class\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "# Create the object\n",
    "tk = TweetTokenizer()\n",
    "\n",
    "# tokenize the data\n",
    "tk.tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: \n",
      "This\n",
      "is\n",
      "some\n",
      "text\n",
      "with\n",
      "punctuation\n",
      ">\n",
      "Let's\n",
      "tokenize\n",
      "it\n",
      "Is\n",
      "it\n",
      "ok\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def custom_tokenizer(text):\n",
    "    return re.split(r\"[.,;?!\\s]+\", text)\n",
    "\n",
    "text = \"This is some text with punctuation > Let's tokenize it. Is it ok?\"\n",
    "\n",
    "tokens = custom_tokenizer(text)\n",
    "\n",
    "print(\"Tokens: \")\n",
    "for token in tokens:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://mitu.co.in/dataset\n",
    "# Download student3.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('student3.tsv')\n",
    "data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roll\tname\tclass\tmarks\tage\n",
      "1\tanil\tTE\t56.77\t22\n",
      "2\tamit\tTE\t59.77\t21\n",
      "3\taniket\tBE\t76.88\t19\n",
      "4\tajinkya\tTE\t69.66\t20\n",
      "5\tasha\tTE\t63.28\t20\n",
      "6\tayesha\tBE\t49.55\t20\n",
      "7\tamar\tBE\t65.34\t19\n",
      "8\tamita\tBE\t68.33\t23\n",
      "9\tamol\tTE\t56.75\t20\n",
      "10\tanmol\tBE\t78.66\t21\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['roll', 'name', 'class', 'marks', 'age'],\n",
       " [1, 'anil', 'TE', 56.77, 22],\n",
       " [2, 'amit', 'TE', 59.77, 21],\n",
       " [3, 'aniket', 'BE', 76.88, 19],\n",
       " [4, 'ajinkya', 'TE', 69.66, 20],\n",
       " [5, 'asha', 'TE', 63.28, 20],\n",
       " [6, 'ayesha', 'BE', 49.55, 20],\n",
       " [7, 'amar', 'BE', 65.34, 19],\n",
       " [8, 'amita', 'BE', 68.33, 23],\n",
       " [9, 'amol', 'TE', 56.75, 20],\n",
       " [10, 'anmol', 'BE', 78.66, 21],\n",
       " ['']]"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newdata = []\n",
    "for x in data.split('\\n'):\n",
    "    inner_list = []\n",
    "    for y in x.split('\\t'):\n",
    "        if y.isdigit():\n",
    "            inner_list.append(int(y))\n",
    "        elif y.find('.') > 0:\n",
    "            inner_list.append(float(y))\n",
    "        else:\n",
    "            inner_list.append(y)\n",
    "    newdata.append(inner_list)\n",
    "newdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['roll\\tname\\tclass\\tmarks\\tage\\n',\n",
       " '1\\tanil\\tTE\\t56.77\\t22\\n',\n",
       " '2\\tamit\\tTE\\t59.77\\t21\\n',\n",
       " '3\\taniket\\tBE\\t76.88\\t19\\n',\n",
       " '4\\tajinkya\\tTE\\t69.66\\t20\\n',\n",
       " '5\\tasha\\tTE\\t63.28\\t20\\n',\n",
       " '6\\tayesha\\tBE\\t49.55\\t20\\n',\n",
       " '7\\tamar\\tBE\\t65.34\\t19\\n',\n",
       " '8\\tamita\\tBE\\t68.33\\t23\\n',\n",
       " '9\\tamol\\tTE\\t56.75\\t20\\n',\n",
       " '10\\tanmol\\tBE\\t78.66\\t21\\n']"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('student3.tsv', 'r') as file:\n",
    "    data1 = file.readlines()\n",
    "data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['1', 'anil', 'TE', '56.77', '22\\n'],\n",
       " ['2', 'amit', 'TE', '59.77', '21\\n'],\n",
       " ['3', 'aniket', 'BE', '76.88', '19\\n'],\n",
       " ['4', 'ajinkya', 'TE', '69.66', '20\\n'],\n",
       " ['5', 'asha', 'TE', '63.28', '20\\n'],\n",
       " ['6', 'ayesha', 'BE', '49.55', '20\\n'],\n",
       " ['7', 'amar', 'BE', '65.34', '19\\n'],\n",
       " ['8', 'amita', 'BE', '68.33', '23\\n'],\n",
       " ['9', 'amol', 'TE', '56.75', '20\\n'],\n",
       " ['10', 'anmol', 'BE', '78.66', '21\\n']]"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tk = TabTokenizer()\n",
    "# ans = [tk.tokenize(data[i]) for i in range(1, len(data))]\n",
    "# ans\n",
    "# tk.tokenize(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_tokenizer(text):\n",
    "    tk = TabTokenizer()\n",
    "    text = re.split('[\\n]', text)\n",
    "    text = tk.tokenize(text[0])\n",
    "    text = [int(word) if word.isnumeric() else word for word in text]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 'anil', 'TE', '56.77', 22],\n",
       " [2, 'amit', 'TE', '59.77', 21],\n",
       " [3, 'aniket', 'BE', '76.88', 19],\n",
       " [4, 'ajinkya', 'TE', '69.66', 20],\n",
       " [5, 'asha', 'TE', '63.28', 20],\n",
       " [6, 'ayesha', 'BE', '49.55', 20],\n",
       " [7, 'amar', 'BE', '65.34', 19],\n",
       " [8, 'amita', 'BE', '68.33', 23],\n",
       " [9, 'amol', 'TE', '56.75', 20],\n",
       " [10, 'anmol', 'BE', '78.66', 21]]"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans = [custom_tokenizer(data1[i]) for i in range(1, len(data1))]\n",
    "ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
